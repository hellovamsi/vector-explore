# Interfaces

This document describes the on-disk artifacts and CLI interfaces.

## Artifacts

## Layout
Artifacts are stored under a novel-rooted layout:
- `<novel_slug>/<chunk_method>/` for chunk artifacts
- `<novel_slug>/<embed_key>/<chunk_method>/` for embedding artifacts
- `<novel_slug>/<store_name>/<chunk_method>__<embed_key>/` for store index artifacts
- `<novel_slug>/queries/<query_hash>/` for query outputs

### `chunks.jsonl`
One JSON object per chunk:
- `chunk_id`: string (e.g. `fixed_00012`)
- `novel_slug`: `frankenstein|mobydick|pride`
- `method`: `fixed|sentences|semantic`
- `start_char`, `end_char`: ints (best-effort offsets)
- `text`: chunk text (including overlap prefix if applicable)
- `overlap`: `null` or `{ "type": "chars"|"sentences", "size": int }`
- `meta`: method-specific metadata

### `embeddings.jsonl`
One JSON object per embedded chunk:
- `chunk_id`
- `text_sha256`
- `embedding`: `list[float]`
- `dim`: int
- `backend`: `ollama|st|cloud`
- `model`: string
- plus `novel_slug`, `method`, `start_char`, `end_char`, `text`

### `retrieval.jsonl`
One JSON object per retrieved result:
- `rank`: int
- `score`: float (store-dependent)
- `chunk_id`: string
- `stored_vector`: `list[float] | null` (omitted when vectors disabled)
- `text_full`: string

### `vector_to_text.json`
- `blurb`: educational explanation (lossy)
- `keywords`: list[str]
- `neighbors`: list of `{id, score, text}`
- `summary`: optional string

### `answer.md`
The final answer generated by the LLM.

### `llm_answer_input.json`
Input metadata for Ollama answer generation:
- endpoint/method/headers/url/timeout
- exact `request_body_raw` and parsed `request_body`
- `prompt_text` and structured `prompt_parts`

### `llm_answer_output.json`
Raw JSON response body returned by Ollama `/api/generate` for answer generation.

### `llm_summary_input.json`
Input metadata for optional Ollama vector-summary generation (only when enabled).

### `llm_summary_output.json`
Raw JSON response body returned by Ollama `/api/generate` for summary generation.

## CLI
Run `vector-explore --help` for full flags.
